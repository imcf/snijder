#!/usr/bin/env python
# -*- coding: utf-8 -*-#

"""
Job spooling engine using the SNIJDER package.
"""

# stdlib imports
import sys
import os
import argparse


import snijder
import snijder.queue
from snijder.jobs import process_jobfile
from snijder.logger import set_verbosity
from snijder.spooler import JobSpooler
from snijder.inotify import JobFileHandler


def parse_arguments():
    """Parse command line arguments."""
    argparser = argparse.ArgumentParser(description=__doc__)
    argparser.add_argument(
        '-s',
        '--spooldir',
        required=True,
        help='spooling directory for jobfiles (e.g. "run/spool/")')
    argparser.add_argument(
        '-c',
        '--config',
        required=False,
        default=None,
        help='GC3Pie config file (default: ~/.gc3/gc3pie.conf)')
    argparser.add_argument(
        '-r',
        '--resource',
        required=False,
        help='GC3Pie resource name')
    argparser.add_argument(
        '-v',
        '--verbosity',
        dest='verbosity',
        action='count',
        default=0)
    try:
        return argparser.parse_args()
    except IOError as err:
        argparser.error(str(err))


def main():
    """Main loop of the SNIJDER Queue Manager."""
    args = parse_arguments()

    # set the loglevel as requested on the commandline
    set_verbosity(args.verbosity)

    # TODO:
    # [x] init spooldirs as staticmethod of spooler
    # [x] remember files in 'cur' directory
    # [x] let spooler then set the JobDescription class variable
    # [ ] let spooler then set the status file of each queue
    # [ ] then check existing files in the 'cur' dir if they belong to any of
    #     our queues, warn otherwise
    # [ ] then process files in the 'new' dir as new ones
    jobqueues = dict()
    jobqueues['hucore'] = snijder.queue.JobQueue()

    try:
        job_spooler = JobSpooler(
            args.spooldir,
            jobqueues['hucore'],
            args.config)
    except RuntimeError as err:
        print '\nERROR instantiating the job spooler: %s\n' % err
        return False

    # select a specific resource if requested on the cmdline:
    if args.resource:
        job_spooler.engine.select_resource(args.resource)

    for qname, queue in jobqueues.iteritems():
        status = os.path.join(job_spooler.dirs['status'], qname + '.json')
        queue.statusfile = status

    # process jobfiles already existing during our startup:
    for jobfile in job_spooler.dirs['newfiles']:
        fname = os.path.join(job_spooler.dirs['new'], jobfile)
        process_jobfile(fname, jobqueues)


    file_handler = JobFileHandler(jobqueues, job_spooler.dirs)

    try:
        # NOTE: spool() is blocking, as it contains the main spooling loop!
        job_spooler.spool()
    finally:
        print 'Cleaning up. Remaining jobs:'
        print jobqueues['hucore'].queue
        file_handler.shutdown()

if __name__ == "__main__":
    sys.exit(main())
